{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whaldsz/deep-learning/blob/main/Disease_Indicators_Prediction_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jScRtiZxMfL9"
      },
      "source": [
        "# Disease Prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA0-wujXMwEE"
      },
      "source": [
        "## Setup and initialization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GKIH3kttpcM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49c1b3c-826d-4116-824f-1c78375e84e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14juYhy7Mp4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8cb550-0e78-4ffd-9c2e-38e3504c473c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "# let's set the random seed to make the results reproducible\n",
        "tf.random.set_seed(299)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install git+https://github.com/tensorflow/docs\n",
        "\n",
        "try:\n",
        "  import tensorflow_docs as tfdocs\n",
        "  import tensorflow_docs.modeling\n",
        "  import tensorflow_docs.plots\n",
        "except:\n",
        "  !pip install git+https://github.com/tensorflow/docs\n",
        "  import tensorflow_docs as tfdocs\n",
        "  import tensorflow_docs.modeling\n",
        "  import tensorflow_docs.plots\n",
        "  "
      ],
      "metadata": {
        "id": "z-8pEjoLrx8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bYBQO6Hoogj"
      },
      "outputs": [],
      "source": [
        "from  IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pathlib\n",
        "import shutil\n",
        "import tempfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNIgOz_Poogj"
      },
      "outputs": [],
      "source": [
        "# currentdir\n",
        "import os\n",
        "\n",
        "logdir = os.path.join(os.getcwd(), \"tensorboard_logs\")\n",
        "shutil.rmtree(logdir, ignore_errors=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmYSJcWMSnZU"
      },
      "source": [
        "## 1. Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx4tROB9MuM8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "fdb273ee-f9aa-464f-a937-638a9d380823"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-470815da0d88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdisease_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/projects/oman-gulf-college/dataset/Disease_Prediction/Training.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdisease_testing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/projects/oman-gulf-college/dataset/Disease_Prediction/Testing.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdisease_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/projects/oman-gulf-college/dataset/Disease_Prediction/Training.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "disease_training = pd.read_csv('/content/drive/MyDrive/projects/oman-gulf-college/dataset/Disease_Prediction/Training.csv')\n",
        "disease_testing = pd.read_csv('/content/drive/MyDrive/projects/oman-gulf-college/dataset/Disease_Prediction/Testing.csv')\n",
        "disease_training.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYkWJhNtXhp0"
      },
      "source": [
        "## 3 Remove last column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZghc-CaSjkK"
      },
      "outputs": [],
      "source": [
        "disease_training.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1v4EZLlaYXl"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYpm082zTDYz"
      },
      "outputs": [],
      "source": [
        "disease_training.drop('Unnamed: 133', inplace=True, axis=1)\n",
        "\n",
        "disease_training.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrU739NTTQHL"
      },
      "outputs": [],
      "source": [
        "#disease_training.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg6O2vb5r_HE"
      },
      "source": [
        "## Convert category to numeric values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWF_gDOVsDEB"
      },
      "outputs": [],
      "source": [
        "#get class labels\n",
        "\n",
        "class_names = np.unique(disease_training.prognosis)\n",
        "disease_training.prognosis = pd.Categorical(disease_training.prognosis)\n",
        "disease_testing.prognosis = pd.Categorical(disease_testing.prognosis)\n",
        "class_names.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47Kx89y1tlDs"
      },
      "outputs": [],
      "source": [
        "#disease_training.prognosis.cat.codes\n",
        "#disease_training\n",
        "#disease_testing.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV6x9-0IactI"
      },
      "source": [
        "## Separate Features and Label - Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz5pI7qjHLlZ"
      },
      "source": [
        "### Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ats9rmYNaiES"
      },
      "outputs": [],
      "source": [
        "X = disease_training.drop('prognosis', axis=1)\n",
        "y = disease_training.prognosis.cat.codes\n",
        "np.unique(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfkFrMKeHOtO"
      },
      "source": [
        "### Unseen Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_32AZ6XnHTZc"
      },
      "outputs": [],
      "source": [
        "X_unseen = disease_testing.drop('prognosis', axis=1)\n",
        "y_unseen = disease_testing.prognosis.cat.codes\n",
        "np.unique(X_unseen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkk4a6TO3ExZ"
      },
      "source": [
        "## Split into Training & Validation Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYRL_KPNcDcr"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=74)\n",
        "#print(X_train.shape)\n",
        "#print(y_train.shape)\n",
        "#print(X_test.shape)\n",
        "#print(y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrD6Ftlu6hSk"
      },
      "outputs": [],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9H6DlOGoogs"
      },
      "outputs": [],
      "source": [
        "# Number of features\n",
        "FEATURES = 132\n",
        "FEATURES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqzUER9BjKE2"
      },
      "source": [
        "## 2. Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4ecoazvoogt"
      },
      "source": [
        "### Training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEQeBn4loogt"
      },
      "outputs": [],
      "source": [
        "FEATURES=X_train.shape[1]\n",
        "N_VALIDATION = X_train.shape[0] *.2 #int(1e3)\n",
        "N_TRAIN = X_train.shape[0]*.8 #int(1e4)\n",
        "BUFFER_SIZE = int(100)\n",
        "BATCH_SIZE = 50\n",
        "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE\n",
        "\n",
        "[FEATURES, N_VALIDATION, N_TRAIN, BUFFER_SIZE, BATCH_SIZE, STEPS_PER_EPOCH]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVAIi_Azoogt"
      },
      "source": [
        "### Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSiaCCIMFAzn"
      },
      "source": [
        "### Find the ideal learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-SVriZKoogu"
      },
      "outputs": [],
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "  0.001,\n",
        "  decay_steps=STEPS_PER_EPOCH*100,\n",
        "  decay_rate=1,\n",
        "  staircase=False)\n",
        "\n",
        "def get_optimizer():\n",
        "  return tf.keras.optimizers.Adam(lr_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6-hY9GRoogu"
      },
      "outputs": [],
      "source": [
        "step = np.linspace(0,100000)\n",
        "lr = lr_schedule(step)\n",
        "plt.figure(figsize = (6,4))\n",
        "plt.plot(step/STEPS_PER_EPOCH, lr)\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.xlabel('Epoch')\n",
        "_ = plt.ylabel('Learning Rate')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#metrics = [\n",
        "#    tfma.metrics.ExampleCount(name='example_count'),\n",
        "#    tf.keras.metrics.SparseCategoricalCrossentropy(\n",
        "#        name='sparse_categorical_crossentropy'),\n",
        "#    tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n",
        "#    tf.keras.metrics.Precision(name='precision', top_k=1),\n",
        "#    tf.keras.metrics.Precision(name='precision', top_k=3),\n",
        "#    tf.keras.metrics.Recall(name='recall', top_k=1),\n",
        "#    tf.keras.metrics.Recall(name='recall', top_k=3),\n",
        "#    tfma.metrics.MultiClassConfusionMatrixPlot(\n",
        "#        name='multi_class_confusion_matrix_plot'),\n",
        "#]\n",
        "\n",
        "METRICS = 'accuracy'\n",
        "LOSS = tf.keras.losses.SparseCategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "XvMlla5mxKVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOADNrUzoogv"
      },
      "source": [
        "### Settings for automation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssXag77Woogv"
      },
      "outputs": [],
      "source": [
        "def get_callbacks(name):\n",
        "  return [\n",
        "    tfdocs.modeling.EpochDots(),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='acc', patience=100),\n",
        "    tf.keras.callbacks.TensorBoard(os.path.join(logdir,name)),\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu3LUWmuoogw"
      },
      "outputs": [],
      "source": [
        "def compile_and_fit(model, name, loss=None, optimizer=None, metrics = None, max_epochs=10000):\n",
        "  if optimizer is None:\n",
        "    optimizer = get_optimizer()\n",
        "\n",
        "  if loss is None:\n",
        "    loss = LOSS\n",
        "  if metrics is None:\n",
        "    metrics = [METRICS]\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=loss,\n",
        "      metrics=metrics\n",
        "  )\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    steps_per_epoch = STEPS_PER_EPOCH,\n",
        "    epochs=max_epochs,\n",
        "    validation_split=0.1,\n",
        "    #validation_data=[X_test, y_test],\n",
        "    callbacks=get_callbacks(name),\n",
        "    verbose=0)\n",
        "  return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBr_8aBuoogw"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrJxzBr8oogw"
      },
      "outputs": [],
      "source": [
        "size_histories = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rKawa53oogw"
      },
      "source": [
        "#### Model 1\n",
        "\n",
        "Simple model with 3 layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYf8vdzhoogw"
      },
      "outputs": [],
      "source": [
        "model1 = tf.keras.Sequential([\n",
        "    layers.Dense(4, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(41, activation=tf.keras.activations.softmax)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY6IX1NHoogx"
      },
      "outputs": [],
      "source": [
        "model1_history = compile_and_fit(\n",
        "    model1, \n",
        "    'models/model1',\n",
        "    loss=LOSS,\n",
        "    metrics=['acc']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "size_histories['model1'] = model1_history"
      ],
      "metadata": {
        "id": "COSrt_JUK1nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotter = tfdocs.plots.HistoryPlotter(metric = 'acc', smoothing_std=10)\n",
        "plotter.plot(size_histories)\n",
        "a = plt.xscale('log')\n",
        "\n",
        "plt.xlim([.01, max(plt.xlim())])\n",
        "plt.ylim([.01, max(plt.ylim())])\n",
        "plt.xlabel(\"Epochs [Log Scale]\")"
      ],
      "metadata": {
        "id": "Q9d9wzCVt9wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model1.evaluate(X_test, y_test)\n",
        "print(f\"Model Loss (Test Set) : {loss}\")\n",
        "print(f\"Model Accuracy (Test Set): {acc}\")"
      ],
      "metadata": {
        "id": "9EeQNCWeMJLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lrs = 1e-4 * (10 ** (tf.range(BATCH_SIZE)/20))\n",
        "#plt.figure(figsize=(6,4))\n",
        "#plt.semilogx(lrs, size_histories['models/model1'].history['loss'])\n",
        "#plt.xlabel(\"Learning Rate\")\n",
        "#plt.ylabel(\"Loss\")\n",
        "#plt.title(\"Learning Rate vs Loss\")"
      ],
      "metadata": {
        "id": "vjUxSuKmwx2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7Z73fwZoogx"
      },
      "source": [
        "#### Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj5DCE-Ooogx"
      },
      "outputs": [],
      "source": [
        "model2 = tf.keras.Sequential([\n",
        "    layers.Dense(4, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(4, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(41, activation=tf.keras.activations.softmax)\n",
        "])\n",
        "\n",
        "#model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "#                optimizer = tf.keras.optimizers.Adam(),\n",
        "#                #metrics=['MultiClassConfusionMatrixPlot'])\n",
        "#                metrics=[\"accuracy\"])\n",
        "\n",
        "#scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10 **(epoch/20))\n",
        "\n",
        "#history = model.fit(X_train, y_train, epochs=40, callbacks=[scheduler])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2_history = compile_and_fit(\n",
        "    model2, \n",
        "    'models/model2',\n",
        "    loss=LOSS,\n",
        "    metrics=['acc']\n",
        ")"
      ],
      "metadata": {
        "id": "09Vn6Fd8K-2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_histories['model2'] = model2_history"
      ],
      "metadata": {
        "id": "LJjQWrWsLDdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotter = tfdocs.plots.HistoryPlotter(metric = 'acc', smoothing_std=10)\n",
        "plotter.plot(size_histories)\n",
        "a = plt.xscale('log')\n",
        "\n",
        "plt.xlim([.01, max(plt.xlim())])\n",
        "plt.ylim([.01, max(plt.ylim())])\n",
        "plt.xlabel(\"Epochs [Log Scale]\")"
      ],
      "metadata": {
        "id": "VEh647TQCbiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNGhoPaToogx"
      },
      "source": [
        "#### Model 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = tf.keras.Sequential([\n",
        "    layers.Dense(64, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(64, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(64, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(41, activation=tf.keras.activations.softmax)\n",
        "])"
      ],
      "metadata": {
        "id": "4gb2L_buEFjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3_history = compile_and_fit(\n",
        "    model3, \n",
        "    'models/model3',\n",
        "    loss=LOSS,\n",
        "    metrics=['acc']\n",
        ")"
      ],
      "metadata": {
        "id": "qeTwK5OPEG7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "size_histories['model3'] = model3_history"
      ],
      "metadata": {
        "id": "VBvPvQdjKhCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotter = tfdocs.plots.HistoryPlotter(metric = 'acc', smoothing_std=10)\n",
        "plotter.plot(size_histories)\n",
        "a = plt.xscale('log')\n",
        "\n",
        "plt.xlim([.01, max(plt.xlim())])\n",
        "plt.ylim([.01, max(plt.ylim())])\n",
        "plt.xlabel(\"Epochs [Log Scale]\")"
      ],
      "metadata": {
        "id": "rG0Ih0YyINsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_wLljy8oogx"
      },
      "source": [
        "#### Model 4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = tf.keras.Sequential([\n",
        "    layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(41, activation=tf.keras.activations.softmax)\n",
        "])\n"
      ],
      "metadata": {
        "id": "w8BlH74rIZQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4_history = compile_and_fit(\n",
        "    model4, \n",
        "    'models/model4',\n",
        "    loss=LOSS,\n",
        "    metrics=['acc']\n",
        ")\n"
      ],
      "metadata": {
        "id": "KahdLp5HId5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "size_histories['model4'] = model4_history"
      ],
      "metadata": {
        "id": "uGxJmLWtKk5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotter = tfdocs.plots.HistoryPlotter(metric = 'acc', smoothing_std=10)\n",
        "plotter.plot(size_histories)\n",
        "a = plt.xscale('log')\n",
        "\n",
        "plt.xlim([.01, max(plt.xlim())])\n",
        "plt.ylim([.01, max(plt.ylim())])\n",
        "plt.xlabel(\"Epochs [Log Scale]\")"
      ],
      "metadata": {
        "id": "A5yIUDcJIhsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5m2qW-Toogy"
      },
      "source": [
        "#### Model 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYdbNB_tGtak"
      },
      "source": [
        "### 3. Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-HLBIrDIi8E"
      },
      "source": [
        "#### Evaluate with test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvcKrMrFGtuW"
      },
      "outputs": [],
      "source": [
        "loss, acc = model1.evaluate(X_test, y_test)\n",
        "print(f\"Model Loss (Test Set) : {loss}\")\n",
        "print(f\"Model Accuracy (Test Set): {acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyqLRD8tIqaO"
      },
      "source": [
        "#### Evaluate with unseen data (Loss vs Accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5berObg_IsXF"
      },
      "outputs": [],
      "source": [
        "loss, acc = model1.evaluate(X_unseen, y_unseen)\n",
        "print(\"Model 1:\")\n",
        "print(f\"Model Loss: {loss}\")\n",
        "print(f\"Model Accuracy: {acc}\")\n",
        "\n",
        "loss, acc = model2.evaluate(X_unseen, y_unseen)\n",
        "print(\"Model 2:\")\n",
        "print(f\"Model Loss: {loss}\")\n",
        "print(f\"Model Accuracy: {acc}\")\n",
        "\n",
        "loss, acc = model3.evaluate(X_unseen, y_unseen)\n",
        "print(\"Model 3:\")\n",
        "print(f\"Model Loss: {loss}\")\n",
        "print(f\"Model Accuracy: {acc}\")\n",
        "\n",
        "loss, acc = model4.evaluate(X_unseen, y_unseen)\n",
        "print(\"Model 4:\")\n",
        "print(f\"Model Loss: {loss}\")\n",
        "print(f\"Model Accuracy: {acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltyV6aoxFPVI"
      },
      "source": [
        "## 3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT95QTFQFZdy"
      },
      "source": [
        "#### Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LASSB5oFR86"
      },
      "outputs": [],
      "source": [
        "### Model 1\n",
        "predictions1 = model1.predict(X_test)\n",
        "\n",
        "predicted1=tf.argmax(predictions1, axis=1)\n",
        "res1= pd.DataFrame({'Test':y_test, 'B':predicted1})\n",
        "\n",
        "summary1 = pd.DataFrame({'Test Set':y_test, 'Predicted':predicted1})\n",
        "summary1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Model 2\n",
        "predictions2 = model2.predict(X_test)\n",
        "\n",
        "predicted2=tf.argmax(predictions2, axis=1)\n",
        "res2= pd.DataFrame({'Test':y_test, 'B':predicted2})\n",
        "\n",
        "summary2 = pd.DataFrame({'Test Set':y_test, 'Predicted':predicted2})\n",
        "summary2\n"
      ],
      "metadata": {
        "id": "6ZcUIuKrbqhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Model 3\n",
        "predictions3 = model3.predict(X_test)\n",
        "\n",
        "predicted3=tf.argmax(predictions3, axis=1)\n",
        "res3= pd.DataFrame({'Test':y_test, 'B':predicted3})\n",
        "\n",
        "summary3 = pd.DataFrame({'Test Set':y_test, 'Predicted':predicted3})\n",
        "summary3\n",
        "\n"
      ],
      "metadata": {
        "id": "yuvkRpvXWLme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzrwsdBoISwK"
      },
      "source": [
        "#### Unseen Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_WPT2W_IUMl"
      },
      "outputs": [],
      "source": [
        "### Unseen set\n",
        "predictions1 = model1.predict(X_unseen)\n",
        "result1=tf.argmax(predictions1, axis=1)\n",
        "res1= pd.DataFrame({'Unseem Set':y_unseen, 'B':result1})\n",
        "\n",
        "predictions2 = model2.predict(X_unseen)\n",
        "result2=tf.argmax(predictions2, axis=1)\n",
        "res2= pd.DataFrame({'Unseem Set':y_unseen, 'B':result2})\n",
        "\n",
        "\n",
        "predictions3 = model3.predict(X_unseen)\n",
        "result3=tf.argmax(predictions3, axis=1)\n",
        "res3= pd.DataFrame({'Unseem Set':y_unseen, 'B':result3})\n",
        "\n",
        "\n",
        "predictions4 = model4.predict(X_unseen)\n",
        "result4=tf.argmax(predictions4, axis=1)\n",
        "res4= pd.DataFrame({'Unseem Set':y_unseen, 'B':result4})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9ZkcLmlFcri"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pexBax2-oog0"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_addons\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred = model1.predict(X_test)\n",
        "\n",
        "\n",
        "#  !pip install git+https://github.com/tensorflow/docs\n",
        "\n",
        "metric = tfa.metrics.MultiLabelConfusionMatrix(num_classes=41)\n",
        "rr=np.argmax(y_pred, axis=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy Score"
      ],
      "metadata": {
        "id": "0yr6BA4xAK6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, rr)"
      ],
      "metadata": {
        "id": "hx4seQN5_zzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multilabel confusion matrix"
      ],
      "metadata": {
        "id": "sv0QfhFQAFAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Actual \\n\", y_test)\n",
        "print(\"\\nPredicted \\n\",rr)\n"
      ],
      "metadata": {
        "id": "t9ABAQDxAXot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, rr)"
      ],
      "metadata": {
        "id": "JgQ1rFBI_6QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(\"Model 1:\")\n",
        "print(classification_report(y_unseen, result1,target_names=class_names))\n",
        "print(\"Model 2:\")\n",
        "print(classification_report(y_unseen, result2,target_names=class_names))\n",
        "print(\"Model 3:\")\n",
        "print(classification_report(y_unseen, result3,target_names=class_names))\n",
        "print(\"Model 4:\")\n",
        "print(classification_report(y_unseen, result4,target_names=class_names))"
      ],
      "metadata": {
        "id": "a7e5qYuCBRSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figsize=[15,15]\n",
        "cm = confusion_matrix(y_test, rr) \n",
        "cm_display = ConfusionMatrixDisplay(cm).plot(ax=plt.subplots(figsize=figsize)[1])\n"
      ],
      "metadata": {
        "id": "XS8XdW-WcZj4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "o5m2qW-Toogy"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}